{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/Documentos/UTN/INA/giar_ina_dev/.venv/lib/python3.12/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "2025-03-29 12:22:11.528844: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-29 12:22:11.534793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743261731.541698   26597 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743261731.543769   26597 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-29 12:22:11.551348: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # If you have more than one GPU, use this to select the one you want to use\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "sys.path.insert(0, \"../packages/python\")\n",
    "from data import utils as data_utils\n",
    "from data import augmentation as data_augmentation\n",
    "from models import cell_segmentation as segmentators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "cuDNN Enabled: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"cuDNN Enabled:\", tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Segmentation Examples\n",
    "\n",
    "This notebook has some examples on how the ´CellMaskGenerator´ class (and its childs) work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Anything Model (SAM)\n",
    "\n",
    "Download checkpoint files here: https://pypi.org/project/segment-anything-py/#model-checkpoints\n",
    "\n",
    "This models requieres 16 GB of RAM (or VRAM) to work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the model, input data and output location, modify to your structure\n",
    "SAM_CHECKPOINT_PATH = \"../models/SAM/sam_vit_h_4b8939.pth\"\n",
    "IMAGE_PATH = \"../media/Onion-Cell-Merged-v6.v1i.coco/valid/\"\n",
    "OUTPUT = \"../output/\"\n",
    "CSV_PATH = os.path.join(OUTPUT, \"cropped_cells_onion_v3/data/valid\")\n",
    "CROPPED_OUTPUT = os.path.join(OUTPUT, \"cropped_cells_onion_v3/media/valid\")\n",
    "MODELS_PATH = '../models/'\n",
    "# Select the devicce: \n",
    "# \"cuda\" : Will use the NVIDIA GPU\n",
    "# \"cpu\" : Will use the... CPU\n",
    "DEVICE_USE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "cmg = segmentators.SAMCellMaskGenerator(SAM_CHECKPOINT_PATH, model_type = 'vit_h', device = DEVICE_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [06:03<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "# Apply segmentation to the whole\n",
    "data_utils.dataset_cell_segmentation(cmg, IMAGE_PATH, CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop nucleai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the output csv with the bbox info, the segmentations are cropped from the original image and stored in CROPPED_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [00:04<00:00, 29.23it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(CROPPED_OUTPUT):\n",
    "    os.makedirs(CROPPED_OUTPUT)\n",
    "\n",
    "for file in tqdm(sorted(os.listdir(IMAGE_PATH))):\n",
    "    image_name = os.fsdecode(file)\n",
    "    image_base_name, _ = os.path.splitext(image_name)\n",
    "    image = os.path.join(IMAGE_PATH, image_name)\n",
    "    csv_path = os.path.join(CSV_PATH, image_base_name + '.csv')\n",
    "    cmg.crop_cells(image_path=image, masks_path=csv_path, output_dir=CROPPED_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the target folder and the new folder to recreate\n",
    "IMAGE_PATH = '../output/cropped_cells_onion_v3/media/'\n",
    "TARGET_FOLDER = '../output/dataset_test/test_roboflow/cells'  # Replace with the actual path to your target folder\n",
    "NEW_FOLDER = '../output/dataset_test/test_roboflow_v3/cells'  # Replace with the desired output folder\n",
    "\n",
    "# Create the new folder if it doesn't exist\n",
    "if not os.path.exists(NEW_FOLDER):\n",
    "    os.makedirs(NEW_FOLDER)\n",
    "\n",
    "# Get the list of images in the target folder\n",
    "target_images = os.listdir(TARGET_FOLDER)\n",
    "\n",
    "# Search for each image in IMAGE_PATH and copy it to the new folder\n",
    "for root, dirs, files in os.walk(IMAGE_PATH):\n",
    "    for file in files:\n",
    "        if file in target_images and file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            source_path = os.path.join(root, file)\n",
    "            destination_path = os.path.join(NEW_FOLDER, file)\n",
    "            shutil.copy(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:21<00:00, 40.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# Apply segmentation to the whole\n",
    "data_utils.dataset_cell_segmentation(cmg, \"../media/test/\", \"../media/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image =  '../media/test/005_00020.jpg'\n",
    "df = pd.read_csv(\"../media/test.csv\") # sam_uploaded_out\n",
    "\n",
    "bboxes_sam_not = [tuple(map(int, row)) for row in df[['x', 'y', 'w', 'h']].values]\n",
    "img = cv2.imread(image,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "x, y, w, h = (2663, 144, 200, 200)\n",
    "x1, y1 = int(x), int(y)\n",
    "x2, y2 = int(x + w), int(y + h)\n",
    "cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 0), 3)\n",
    "\n",
    "x, y, w, h = (2617, 119, 292, 250)\n",
    "x1, y1 = int(x), int(y)\n",
    "x2, y2 = int(x + w), int(y + h)\n",
    "cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"005_00020.jpg\"\n",
    "image_path =  f'../media/uploaded/{image_name}'\n",
    "image = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "df = pd.read_csv(\"../output/sam_uploaded_out/005_00020.csv\") # sam_uploaded_out\n",
    "df_bbox = df[df['image'] == image_name][['x', 'y', 'w', 'h']]      \n",
    "\n",
    "# Iterate over the bounding boxes and crop the image\n",
    "for _, row in df_bbox.iterrows():\n",
    "    x1, y1, w1, h1 = row\n",
    "    x, y, w, h = cmg._adjust_bbox(row['x'], row['y'], row['w'], row['h'], 200 * 200)\n",
    "\n",
    "    cv2.rectangle(image, (x1, y1), (x1 + w1, y1 + h1), (0, 0, 0), 3)\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 0), 3)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmg.crop_cells(image_path=image_path, masks_path=\"../output/sam_uploaded_out/005_00020.csv\", output_dir=\"../media/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using albumination, the dataset formed by the cropped images in CROPPED_OUTPUT is augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(path):\n",
    "    \"\"\"\n",
    "    Gets a list of all files within a specified path, including subdirectories.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the directory.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of file paths.\n",
    "    \"\"\"\n",
    "\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_list.append(file_path)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image):\n",
    "    # Define the augmentation pipeline\n",
    "    transform = A.Compose([\n",
    "        A.Rotate(limit=(-180, 180), p=1),\n",
    "        A.HorizontalFlip(),\n",
    "        A.VerticalFlip(),\n",
    "        A.RandomBrightnessContrast(p=0.4),\n",
    "        A.RandomGamma(p=1, gamma_limit=(60, 110)),\n",
    "        A.GaussNoise(p=0.2)\n",
    "    ])\n",
    "\n",
    "    augmented_image = transform(image=image)['image']\n",
    "\n",
    "    return augmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_images = get_file_list(CROPPED_OUTPUT)\n",
    "\n",
    "for idx, image_path in enumerate(output_images):\n",
    "    print(f\"Image: {idx + 1}/{len(output_images)}\", end='\\r')\n",
    "\n",
    "    # Read and augment image\n",
    "    image = cv2.imread(image_path)\n",
    "    augmented_image = augment_image(image)\n",
    "\n",
    "    image_name = os.path.basename(image_path)\n",
    "    path = os.path.dirname(image_path)\n",
    "    base_name, ext = os.path.splitext(image_name)\n",
    "\n",
    "    # Define augmented image name\n",
    "    new_name = f\"{base_name}_augmented{ext}\"\n",
    "    counter = 1\n",
    "    while os.path.exists(os.path.join(path, new_name)):\n",
    "        new_name = f\"{base_name}_augmented_{counter}{ext}\"\n",
    "        counter += 1\n",
    "\n",
    "    # Save the augmented image\n",
    "    cv2.imwrite(os.path.join(path, new_name), augmented_image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply detected bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping throught each image in CROPPED_OUTPUT it matches the image with the cell id in the CSV_PATH file to find the original image from where the cell was cropped. Then with the specified model it checks whether the image is a cell or not and if it is, with the bbox data in the csv it draws all the cells their corresponding bbox the in the original image and stores it in ../detected_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = os.path.join(OUTPUT, \"sam_out_onion.csv\")\n",
    "segmentators.CellMaskGenerator.bbox_applier(model_path=os.path.join(MODELS_PATH,'VGG19.keras'), csv_path=CSV_PATH, cells_path=CROPPED_OUTPUT, images_path=IMAGE_PATH)#, encoder_path=os.path.join(MODELS_PATH, 'encoder2.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_grid(folder_path, rows=4, cols=4):\n",
    "  \"\"\"\n",
    "  Plots a grid of images from a given folder.\n",
    "\n",
    "  Args:\n",
    "    folder_path: Path to the folder containing images.\n",
    "    rows: Number of rows in the grid.\n",
    "    cols: Number of columns in the grid.\n",
    "  \"\"\"\n",
    "\n",
    "  fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "  # Adjust spacing between subplots\n",
    "  fig.subplots_adjust(hspace=0.01, wspace=0.1)  # Reduce spacing\n",
    "\n",
    "  image_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "  for ax, image_path in zip(axes.flat, image_paths):\n",
    "    img = plt.imread(image_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(os.path.basename(image_path))\n",
    "    ax.axis('off')\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "# Example usage:\n",
    "folder_path = '../detected_cells/'\n",
    "plot_image_grid(folder_path, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../media/Onion-Cell-Merged-v6.v1i.coco/'\n",
    "dataset = 'train'\n",
    "images = os.listdir(f'{PATH}/{dataset}')\n",
    "\n",
    "for image in images:\n",
    "    image_data = image.split('_')\n",
    "    if image_data[0] == 'annotation':\n",
    "        continue\n",
    "    new_name = image_data[0] + '_' + image_data[1] + '.png'\n",
    "    os.rename(f'{PATH}{dataset}/{image}', f'{PATH}{dataset}/{new_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATASET = 'valid'\n",
    "PATH = f'../media/Onion-Cell-Merged-v6.v1i.coco/{DATASET}/'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(f'{PATH}annotations_coco.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Process the 'file_name' field in each image\n",
    "for image in data['images']:\n",
    "    original_name = image['file_name']\n",
    "    \n",
    "    # Extract the first letter and the number between underscores\n",
    "    parts = original_name.split('_')\n",
    "    if len(parts) >= 2:\n",
    "        new_name = f\"{parts[0]}_{parts[1]}.png\"\n",
    "        image['file_name'] = new_name\n",
    "\n",
    "# Save the modified JSON back to the file\n",
    "with open(f'{PATH}annotations_coco_v2.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
