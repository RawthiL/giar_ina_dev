{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Segmentation\n",
    "\n",
    "This notebook uses SAM to segment the given images and stores the segmentation information in a csv file for each image\n",
    "\n",
    "## Segment Anything Model (SAM)\n",
    "\n",
    "Download checkpoint files here: https://pypi.org/project/segment-anything-py/#model-checkpoints\n",
    "\n",
    "This models requieres 16 GB of RAM (or VRAM) to work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # If you have more than one GPU, use this to select the one you want to use\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.insert(0, \"../../packages/python\")\n",
    "from data import utils as data_utils\n",
    "from data import augmentation as data_augmentation\n",
    "from models import cell_segmentation as segmentators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "cuDNN Enabled: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"cuDNN Enabled:\", tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../../\")\n",
    "from config import MODELS_PATH, MEDIA_PATH\n",
    "\n",
    "# Dataset selection\n",
    "DATASET = 'onion_cell_merged'#'ina' # Change this to 'onion_cell_merged' if needed\n",
    "DATASET_SECTION = 'test' # Change this to 'train', 'valida' or 'test' for 'onion_cell_merged', empty for 'ina'\n",
    "\n",
    "# Specific paths\n",
    "SAM_CHECKPOINT_PATH = os.path.join(MODELS_PATH, \"sam/sam_vit_h_4b8939.pth\")\n",
    "IMAGE_PATH = os.path.join(MEDIA_PATH, f\"images/{DATASET}/images/{DATASET_SECTION}\")\n",
    "CSV_PATH = os.path.join(MEDIA_PATH, f\"cropped_images/{DATASET}/data/{DATASET_SECTION}\")\n",
    "\n",
    "#CROPPED_OUTPUT = os.path.join(MEDIA_PATH, f\"cropped_images/{DATASET}/images\")\n",
    "\n",
    "# Select the devicce: \n",
    "# \"cuda\" : Will use the NVIDIA GPU\n",
    "# \"cpu\" : Will use the... CPU\n",
    "DEVICE_USE = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "cmg = segmentators.SAMCellMaskGenerator(SAM_CHECKPOINT_PATH, model_type = 'vit_h', device = DEVICE_USE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply segmentation to the whole\n",
    "Using SAM to segment the images in IMAGE_PATH the output will be stored in separeted csv files in CSV_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–‹         | 4/63 [00:12<03:06,  3.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply segmentation to the whole\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdata_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_cell_segmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMAGE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/UTN/INA/giar_ina_dev/notebooks/dataset_generation/../../packages/python/data/utils.py:527\u001b[0m, in \u001b[0;36mdataset_cell_segmentation\u001b[0;34m(segment_model, images_path, output_csv)\u001b[0m\n\u001b[1;32m    525\u001b[0m image_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfsdecode(file)\n\u001b[1;32m    526\u001b[0m image_base_name, _ \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(file)\n\u001b[0;32m--> 527\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43msegment_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mask_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_base_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_csv, image_base_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    530\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(path, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documentos/UTN/INA/giar_ina_dev/notebooks/dataset_generation/../../packages/python/models/cell_segmentation.py:214\u001b[0m, in \u001b[0;36mSAMCellMaskGenerator.get_mask_metadata\u001b[0;34m(self, image, image_name)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_mask_metadata\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39marray, image_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miamge\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    Takes a grayscale image and produces a dataframe containing all the data of the segmentation process\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    \"area\", \"x\", \"y\", \"w\", \"h\", \"bbox_area\", \"image\", \"cell_id\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     sam_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     filtered_sam_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_masks(sam_result)\n\u001b[1;32m    216\u001b[0m     masks_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masks_to_df(filtered_sam_result, image_name)\n",
      "File \u001b[0;32m~/Documentos/UTN/INA/giar_ina_dev/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/UTN/INA/giar_ina_dev/.venv/lib/python3.12/site-packages/segment_anything/automatic_mask_generator.py:163\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator.generate\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m mask_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_mask_region_area \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documentos/UTN/INA/giar_ina_dev/.venv/lib/python3.12/site-packages/segment_anything/automatic_mask_generator.py:206\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._generate_masks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    204\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[0;32m--> 206\u001b[0m     crop_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(crop_data)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Remove duplicate masks between crops\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/UTN/INA/giar_ina_dev/.venv/lib/python3.12/site-packages/segment_anything/automatic_mask_generator.py:245\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[0m\n\u001b[1;32m    243\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (points,) \u001b[38;5;129;01min\u001b[39;00m batch_iterator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints_per_batch, points_for_image):\n\u001b[0;32m--> 245\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcropped_im_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(batch_data)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m batch_data\n",
      "File \u001b[0;32m~/Documentos/UTN/INA/giar_ina_dev/.venv/lib/python3.12/site-packages/segment_anything/automatic_mask_generator.py:318\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_batch\u001b[0;34m(self, points, im_size, crop_box, orig_size)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# Compress to RLE\u001b[39;00m\n\u001b[1;32m    317\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m uncrop_masks(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m], crop_box, orig_h, orig_w)\n\u001b[0;32m--> 318\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrles\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmask_to_rle_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmasks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documentos/UTN/INA/giar_ina_dev/.venv/lib/python3.12/site-packages/segment_anything/utils/amg.py:118\u001b[0m, in \u001b[0;36mmask_to_rle_pytorch\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Compute change indices\u001b[39;00m\n\u001b[1;32m    117\u001b[0m diff \u001b[38;5;241m=\u001b[39m tensor[:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m^\u001b[39m tensor[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m change_indices \u001b[38;5;241m=\u001b[39m \u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Encode run length\u001b[39;00m\n\u001b[1;32m    121\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply segmentation to the whole\n",
    "data_utils.dataset_cell_segmentation(cmg, IMAGE_PATH, CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop nucleai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the output csv with the bbox info, the segmentations are cropped from the original image and stored in CROPPED_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 392/392 [00:14<00:00, 27.91it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(CROPPED_OUTPUT):\n",
    "    os.makedirs(CROPPED_OUTPUT)\n",
    "\n",
    "for file in tqdm(sorted(os.listdir(IMAGE_PATH))):\n",
    "    image_name = os.fsdecode(file)\n",
    "    image_base_name, _ = os.path.splitext(image_name)\n",
    "    image = os.path.join(IMAGE_PATH, image_name)\n",
    "    csv_path = os.path.join(CSV_PATH, image_base_name + '.csv')\n",
    "    cmg.crop_cells(image_path=image, masks_path=csv_path, output_dir=CROPPED_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using albumination, the dataset formed by the cropped images in CROPPED_OUTPUT is augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(path):\n",
    "    \"\"\"\n",
    "    Gets a list of all files within a specified path, including subdirectories.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the directory.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of file paths.\n",
    "    \"\"\"\n",
    "\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_list.append(file_path)\n",
    "    return file_list\n",
    "\n",
    "def augment_image(image):\n",
    "    # Define the augmentation pipeline\n",
    "    transform = A.Compose([\n",
    "        A.Rotate(limit=(-180, 180), p=1),\n",
    "        A.HorizontalFlip(),\n",
    "        A.VerticalFlip(),\n",
    "        A.RandomBrightnessContrast(p=0.4),\n",
    "        A.RandomGamma(p=1, gamma_limit=(60, 110)),\n",
    "        A.GaussNoise(p=0.2)\n",
    "    ])\n",
    "\n",
    "    augmented_image = transform(image=image)['image']\n",
    "\n",
    "    return augmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_images = get_file_list(CROPPED_OUTPUT)\n",
    "\n",
    "for idx, image_path in enumerate(output_images):\n",
    "    print(f\"Image: {idx + 1}/{len(output_images)}\", end='\\r')\n",
    "\n",
    "    # Read and augment image\n",
    "    image = cv2.imread(image_path)\n",
    "    augmented_image = augment_image(image)\n",
    "\n",
    "    image_name = os.path.basename(image_path)\n",
    "    path = os.path.dirname(image_path)\n",
    "    base_name, ext = os.path.splitext(image_name)\n",
    "\n",
    "    # Define augmented image name\n",
    "    new_name = f\"{base_name}_augmented{ext}\"\n",
    "    counter = 1\n",
    "    while os.path.exists(os.path.join(path, new_name)):\n",
    "        new_name = f\"{base_name}_augmented_{counter}{ext}\"\n",
    "        counter += 1\n",
    "\n",
    "    # Save the augmented image\n",
    "    cv2.imwrite(os.path.join(path, new_name), augmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply detected bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping throught each image in CROPPED_OUTPUT it matches the image with the cell id in the CSV_PATH file to find the original image from where the cell was cropped. Then with the specified model it checks whether the image is a cell or not and if it is, with the bbox data in the csv it draws all the cells their corresponding bbox the in the original image and stores it in ../detected_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = os.path.join(OUTPUT, \"sam_out_onion.csv\")\n",
    "segmentators.CellMaskGenerator.bbox_applier(model_path=os.path.join(MODELS_PATH,'VGG19.keras'), csv_path=CSV_PATH, cells_path=CROPPED_OUTPUT, images_path=IMAGE_PATH)#, encoder_path=os.path.join(MODELS_PATH, 'encoder2.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_grid(folder_path, rows=4, cols=4):\n",
    "  \"\"\"\n",
    "  Plots a grid of images from a given folder.\n",
    "\n",
    "  Args:\n",
    "    folder_path: Path to the folder containing images.\n",
    "    rows: Number of rows in the grid.\n",
    "    cols: Number of columns in the grid.\n",
    "  \"\"\"\n",
    "\n",
    "  fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "  # Adjust spacing between subplots\n",
    "  fig.subplots_adjust(hspace=0.01, wspace=0.1)  # Reduce spacing\n",
    "\n",
    "  image_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "  for ax, image_path in zip(axes.flat, image_paths):\n",
    "    img = plt.imread(image_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(os.path.basename(image_path))\n",
    "    ax.axis('off')\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "# Example usage:\n",
    "folder_path = '../detected_cells/'\n",
    "plot_image_grid(folder_path, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../media/Onion-Cell-Merged-v6.v1i.coco/'\n",
    "dataset = 'train'\n",
    "images = os.listdir(f'{PATH}/{dataset}')\n",
    "\n",
    "for image in images:\n",
    "    image_data = image.split('_')\n",
    "    if image_data[0] == 'annotation':\n",
    "        continue\n",
    "    new_name = image_data[0] + '_' + image_data[1] + '.png'\n",
    "    os.rename(f'{PATH}{dataset}/{image}', f'{PATH}{dataset}/{new_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATASET = 'valid'\n",
    "PATH = f'../media/Onion-Cell-Merged-v6.v1i.coco/{DATASET}/'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(f'{PATH}annotations_coco.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Process the 'file_name' field in each image\n",
    "for image in data['images']:\n",
    "    original_name = image['file_name']\n",
    "    \n",
    "    # Extract the first letter and the number between underscores\n",
    "    parts = original_name.split('_')\n",
    "    if len(parts) >= 2:\n",
    "        new_name = f\"{parts[0]}_{parts[1]}.png\"\n",
    "        image['file_name'] = new_name\n",
    "\n",
    "# Save the modified JSON back to the file\n",
    "with open(f'{PATH}annotations_coco_v2.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the target folder and the new folder to recreate\n",
    "IMAGE_PATH = '../output/cropped_cells_onion_v4/media/'\n",
    "TARGET_FOLDER = '../output/dataset_test/test_roboflow/not'  # Replace with the actual path to your target folder\n",
    "NEW_FOLDER = '../output/dataset_test/test_roboflow_v4/not'  # Replace with the desired output folder\n",
    "\n",
    "# Create the new folder if it doesn't exist\n",
    "if not os.path.exists(NEW_FOLDER):\n",
    "    os.makedirs(NEW_FOLDER)\n",
    "\n",
    "# Get the list of images in the target folder\n",
    "target_images = os.listdir(TARGET_FOLDER)\n",
    "\n",
    "# Search for each image in IMAGE_PATH and copy it to the new folder\n",
    "for root, dirs, files in os.walk(IMAGE_PATH):\n",
    "    for file in files:\n",
    "        if file in target_images and file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            source_path = os.path.join(root, file)\n",
    "            destination_path = os.path.join(NEW_FOLDER, file)\n",
    "            shutil.copy(source_path, destination_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
