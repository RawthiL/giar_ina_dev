{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import keras\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "cuDNN Enabled: True\n",
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPU Available:\", gpus)\n",
    "print(\"cuDNN Enabled:\", tf.test.is_built_with_cuda())\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = '../../media/data/input_color/'\n",
    "\n",
    "sys.path.insert(0, \"../../\")\n",
    "from config import MEDIA_PATH, CROPPED_PATH, MODELS_PATH\n",
    "\n",
    "#Configuration\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Paths\n",
    "CSV_PATH = os.path.join(CROPPED_PATH, 'ina', 'data')\n",
    "INA_CROPS_PATH = os.path.join(CROPPED_PATH, 'ina', 'images')\n",
    "RF_CROPS_PATH = os.path.join(CROPPED_PATH, 'onion_cell_merged', 'images')\n",
    "MODEL_PATH = os.path.join(MODELS_PATH, 'supervised', 'supervised_Encoder_SSIM+MAE3.keras')\n",
    "JSON_PATH = os.path.join(MEDIA_PATH, 'images', 'ina', 'tagged_images', 'corte-27-02-2024.json')\n",
    "OUTPUT_CLASSES_PATH = os.path.join(MEDIA_PATH, 'cropped_images', 'classification')\n",
    "IMAGES_PATH = os.path.join(MEDIA_PATH, 'images', 'ina', 'tagged_images', 'input')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco_bbox(data, id):\n",
    "  \"\"\"\n",
    "  Given a list of objects it returns a list of bounding boxes for the given id.\n",
    "\n",
    "  Args:\n",
    "    data: list of objects to search from.\n",
    "    id: id of the objetct to search.\n",
    "\n",
    "  Returns:\n",
    "    A list of bounding boxes.\n",
    "  \"\"\"\n",
    "  bboxes = [cell['bbox'] for cell in data['annotations'] if cell['image_id'] == id]\n",
    "  classes = [cell['attributes']['Fase'] for cell in data['annotations'] if cell['image_id'] == id]\n",
    "  return bboxes, classes\n",
    "\n",
    "def bb_overlap_percentage(box1, box2):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) of two bounding boxes using Shapely.\n",
    "\n",
    "    Args:\n",
    "        box1: A tuple or list containing (x1, y1, width, height) of the first bounding box.\n",
    "        box2: A tuple or list containing (x1, y1, width, height) of the second bounding box.\n",
    "\n",
    "    Returns:\n",
    "        The IoU value, a float between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate box coordinates\n",
    "    x1_min, y1_min, width1, height1 = box1\n",
    "    x1_max = x1_min + width1\n",
    "    y1_max = y1_min + height1\n",
    "    x2_min, y2_min, width2, height2 = box2\n",
    "    x2_max = x2_min + width2\n",
    "    y2_max = y2_min + height2\n",
    "\n",
    "    # Create polygons\n",
    "    poly1 = Polygon([(x1_min, y1_min), (x1_max, y1_min), (x1_max, y1_max), (x1_min, y1_max)])\n",
    "    poly2 = Polygon([(x2_min, y2_min), (x2_max, y2_min), (x2_max, y2_max), (x2_min, y2_max)])\n",
    "\n",
    "    # Calculate intersection and union areas\n",
    "    intersection = poly1.intersection(poly2).area\n",
    "    union = poly1.union(poly2).area\n",
    "\n",
    "    #print(poly1.area, poly2.area, intersection)\n",
    "\n",
    "    return intersection / poly1.area if union > 0 else 0.0\n",
    "\n",
    "def bbox_fully_contained(bbox1, bbox2):\n",
    "    \"\"\"\n",
    "    Checks if bounding box 1 is fully contained within bounding box 2.\n",
    "\n",
    "    Args:\n",
    "        bbox1: A tuple (x1, y1, width1, height1) representing the first bounding box.\n",
    "        bbox2: A tuple (x2, y2, width2, height2) representing the second bounding box.\n",
    "\n",
    "    Returns:\n",
    "        True if bbox1 is fully contained within bbox2, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate coordinates of bounding boxes\n",
    "    x1, y1, w1, h1 = bbox1\n",
    "    x2, y2, w2, h2 = bbox2\n",
    "    bbox1_coords = [(x1, y1), (x1 + w1, y1), (x1 + w1, y1 + h1), (x1, y1 + h1)]\n",
    "    bbox2_coords = [(x2, y2), (x2 + w2, y2), (x2 + w2, y2 + h2), (x2, y2 + h2)]\n",
    "\n",
    "    # Create Shapely polygons\n",
    "    poly1 = Polygon(bbox1_coords)\n",
    "    poly2 = Polygon(bbox2_coords)\n",
    "\n",
    "    # Check if bbox1 is fully contained within bbox2\n",
    "    return poly1.within(poly2)\n",
    "\n",
    "def bbox_intercept(bbox, bbox_list, threshold=0):\n",
    "  for idx, bbox_target in enumerate(bbox_list):\n",
    "    if threshold > 0: # 0.75\n",
    "      iou = bb_overlap_percentage(bbox, bbox_target)\n",
    "      if iou >= threshold:\n",
    "        return True, idx\n",
    "\n",
    "    else:\n",
    "      contained = bbox_fully_contained(bbox, bbox_target)\n",
    "      if contained:\n",
    "        return True, idx\n",
    "\n",
    "  return False, -1\n",
    "\n",
    "def find_image_name_and_id(data, image_name):\n",
    "  \"\"\"\n",
    "  Given a list of objects it searches the file_name from the id.\n",
    "\n",
    "  Args:\n",
    "    data: list of objects to search from.\n",
    "    id: id of the file name to return.\n",
    "\n",
    "  Returns:\n",
    "    The file name of the image id.\n",
    "  \"\"\"\n",
    "  #First i have to check wether the image_name is an id or a file name (ex: 331 or 004_000001.jpg)\n",
    "  if(image_name.isnumeric()):\n",
    "    id = int(image_name)\n",
    "    for img in data['images']:\n",
    "        if img['id'] == id:\n",
    "            real_name, _ = os.path.splitext(img['file_name'])\n",
    "            return real_name, id\n",
    "  else:\n",
    "    real_name = image_name\n",
    "    for img in data['images']:\n",
    "        if real_name in img['file_name']:\n",
    "            id = int(img['id'])\n",
    "            return real_name, id\n",
    "\n",
    "def process_images_in_batches(string_list, batch_size=10):\n",
    "  \"\"\"\n",
    "  Processes a list of strings in batches of a specified size.\n",
    "\n",
    "  Args:\n",
    "    string_list: The list of strings to process.\n",
    "    batch_size: The size of each batch.\n",
    "\n",
    "  Yields:\n",
    "    A batch of strings.\n",
    "  \"\"\"\n",
    "  for i in range(0, len(string_list), batch_size):\n",
    "    batch_num = i // batch_size + 1  # Calculate batch number (1-indexed)\n",
    "    yield batch_num, string_list[i:i + batch_size]\n",
    "\n",
    "def predict_cell(model, image_path, images_batch, color_type):\n",
    "  \"\"\"\n",
    "  Given an image batch it returns the predictions of the batch with the given model.\n",
    "\n",
    "  Args:\n",
    "    model: keras model to use.\n",
    "    image_path: path to the folder where the images are.\n",
    "    images_batch: list of the image names to include in the batch\n",
    "\n",
    "  Returns:\n",
    "    A list of predictions.\n",
    "  \"\"\"\n",
    "\n",
    "  images = []\n",
    "  for image in images_batch:\n",
    "      img = cv.imread(os.path.join(image_path,image), color_type)\n",
    "      img = cv.resize(img, (128, 128))\n",
    "      img = img / 255.0\n",
    "      images.append(img)\n",
    "  \n",
    "  batch = np.stack(images)\n",
    "  if color_type == cv.IMREAD_GRAYSCALE:\\\n",
    "    # Add missing channel\n",
    "    batch = np.expand_dims(batch, axis=-1).astype(np.float32)\n",
    "\n",
    "  prediction = model.predict(batch, verbose=0)\n",
    "  prediction = tf.nn.softmax(prediction, axis=-1)\n",
    "  return prediction\n",
    "\n",
    "def extract_cell_id(filename):\n",
    "    # Assumes filename format: <prefix>_<cell_id>.png\n",
    "    return int(os.path.splitext(filename)[0].split('_')[-1])\n",
    "\n",
    "def list_files(directory):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            all_files.append(os.path.join(root, file))\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of elements to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = sorted(os.listdir(CSV_PATH)) #Paths to the csv of SAM detections of each image\n",
    "ina_crops = list_files(INA_CROPS_PATH) #Paths to the ina_crops made from SAM detection of the full_images\n",
    "rf_crops = list_files(RF_CROPS_PATH) #Paths to the ina_crops made from SAM detection of the full_images\n",
    "# images = sorted(os.listdir(IMAGES_PATH)) #full_images from where the ina_crops are made\n",
    "with open(JSON_PATH, 'r') as f: #json with the information of the filename of the images\n",
    "    data = json.load(f)\n",
    "\n",
    "model = keras.models.load_model(MODEL_PATH)\n",
    "if model.input.shape[-1] == 1:\n",
    "    color_type = cv.IMREAD_GRAYSCALE\n",
    "else:\n",
    "    color_type = cv.IMREAD_COLOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_ids = set(list(annotation[\"image_id\"] for annotation in data['annotations']))\n",
    "tagged_images = list(find_image_name_and_id(data, str(id))[0] for id in tagged_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3032/3031\r"
     ]
    }
   ],
   "source": [
    "all_crops = ina_crops + rf_crops\n",
    "\n",
    "all_cell_crops = []\n",
    "\n",
    "for idx, batch in process_images_in_batches(all_crops, batch_size=BATCH_SIZE): #Read the images in batch_size batches\n",
    "    print(f\"Batch {idx}/{int(len(all_crops)/BATCH_SIZE)}\", end='\\r')\n",
    "\n",
    "    batch_prediction = predict_cell(model, image_path=INA_CROPS_PATH, images_batch=batch, color_type=color_type)\n",
    "\n",
    "    is_cell = 1-np.argmax(batch_prediction, axis=1).astype(bool)\n",
    "\n",
    "    for idx, crop in enumerate(batch):\n",
    "        if is_cell[idx]:\n",
    "            all_cell_crops.append(crop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_crops = []\n",
    "untagged_crops = all_cell_crops\n",
    "\n",
    "for tagged_image in tagged_images:\n",
    "    to_move = [cell_crop for cell_crop in untagged_crops if tagged_image in cell_crop]\n",
    "    for cell_crop in to_move:\n",
    "        untagged_crops.remove(cell_crop)\n",
    "        test_crops.append(cell_crop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "from collections import defaultdict\n",
    "\n",
    "prefix_to_files = defaultdict(list)\n",
    "\n",
    "for path in untagged_crops:\n",
    "    prefix = os.path.basename(path).split('_')[0]\n",
    "    prefix_to_files[prefix].append(path)\n",
    "\n",
    "train_crops = untagged_crops\n",
    "validation_crops = []\n",
    "\n",
    "# Select and remove 10% from each group\n",
    "for prefix, files in prefix_to_files.items():\n",
    "    n = max(1, int(len(files) * 0.1))  # At least 1 file if group is small\n",
    "    chosen = random.sample(files, n)\n",
    "    validation_crops.extend(chosen)\n",
    "\n",
    "    for f in chosen:\n",
    "        train_crops.remove(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = os.path.join(OUTPUT_CLASSES_PATH, 'train')\n",
    "validation_output = os.path.join(OUTPUT_CLASSES_PATH, 'validation')\n",
    "test_output = os.path.join(OUTPUT_CLASSES_PATH, 'test')\n",
    "\n",
    "os.makedirs(train_output, exist_ok=True)\n",
    "os.makedirs(validation_output, exist_ok=True)\n",
    "os.makedirs(test_output, exist_ok=True)\n",
    "\n",
    "def copy_images_to_folder(image_paths, output):\n",
    "    \"\"\"\n",
    "    Copies images to a folder named after the subset (train/val/test) inside output_base.\n",
    "    Creates the folder if it does not exist.\n",
    "    \"\"\"\n",
    "    for img_path in image_paths:\n",
    "        shutil.copy(img_path, output)\n",
    "\n",
    "copy_images_to_folder(train_crops, train_output)\n",
    "copy_images_to_folder(validation_crops, validation_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_crop in test_crops:\n",
    "    # Extract the base filename without extension\n",
    "    base = os.path.basename(test_crop)\n",
    "    cell_id = base.split('.')[0].split('_')[-1]\n",
    "    image_name = '_'.join(base.split('_')[:2])\n",
    "    _, image_number = find_image_name_and_id(data, image_name= image_name)\n",
    "\n",
    "    df = pd.read_csv(os.path.join(CSV_PATH, f\"{image_name}.csv\"))\n",
    "    df_bbox = df[df['cell_id'] == int(cell_id)]\n",
    "\n",
    "    bbox = df_bbox[['x', 'y', 'w', 'h']].values.flatten().tolist()\n",
    "\n",
    "    bboxes_coco, classes_coco =  get_coco_bbox(data, image_number)\n",
    "\n",
    "    intercept, coco_idx = bbox_intercept(bbox, bboxes_coco, 0.75)\n",
    "\n",
    "    if intercept:\n",
    "        output_dir = os.path.join(test_output, classes_coco[coco_idx])\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        shutil.copy(test_crop, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
